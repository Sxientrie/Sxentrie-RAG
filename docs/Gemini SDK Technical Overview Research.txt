Gemini 2.5 Series: Internal Technical Overview for React/TypeScript Development




1.0 Introduction


The Gemini 2.5 series of models represents a significant advancement in generative AI, providing a powerful and versatile foundation for developing next-generation intelligent applications. This document serves as a definitive internal technical overview for engineers and product managers building with React and TypeScript. Its purpose is to provide a foundational resource detailing the capabilities of the Gemini 2.5 Pro and Gemini 2.5 Flash models, with a specific focus on practical implementation using the official @google/genai Software Development Kit (SDK).
The Gemini 2.5 models, now generally available and production-ready, offer state-of-the-art performance in multimodal understanding, complex reasoning, and high-throughput generation, making them suitable for mission-critical enterprise applications.1 This overview will explore the core architecture of these models, provide a comprehensive reference for the SDK, and present detailed, end-to-end workflows for implementing key features such as real-time streaming responses—a technique conceptually referred to as a
thoughtStream—and multi-modal analysis.
A critical component of this ecosystem is the @google/genai npm package. This SDK is the designated and sole path for accessing the latest Gemini 2.0+ features.2 Its introduction marks a strategic consolidation by Google, which is actively migrating developers from legacy libraries like
@google/generativeai. Support for these older packages is scheduled to end, making the adoption of @google/genai a non-negotiable standard for all new development to avoid technical debt and to leverage the full capabilities of the Gemini 2.5 series.3 By focusing on this modern, unified SDK, our teams can ensure access to the latest innovations, from adaptive thinking to advanced function calling, building on a stable and actively maintained foundation. All examples and workflows within this document are based on this official SDK, with further resources available at the official developer portal,
ai.google.dev.4


2.0 Model Naming and Initialization


Correctly identifying and initializing a Gemini model are the foundational first steps in any development workflow. Using the precise model identifier string is mandatory for all API requests. The stable, production-ready versions of the Gemini 2.5 series have distinct identifiers that developers must use in their code.
Table 2.1: Official Model Identifiers


Model Name
	API Identifier String
	Description
	Gemini 2.5 Pro
	gemini-2.5-pro
	The most powerful and capable model, designed for complex reasoning, deep multimodal understanding, and advanced coding tasks.6
	Gemini 2.5 Flash
	gemini-2.5-flash
	A model optimized for price-performance, engineered for high-throughput, low-latency applications like large-scale summarization and responsive chatbots.1
	

SDK Initialization in TypeScript


To interact with these models, an application must first instantiate the GoogleGenAI client from the @google/genai package. The standard method for server-side applications involves providing an API key, which can be obtained from Google AI Studio. It is a critical security best practice to manage this key as an environment variable and never expose it in client-side code.2
The following TypeScript snippet demonstrates the canonical initialization process:


TypeScript




import { GoogleGenAI } from "@google/genai";

// 1. Retrieve the API key from environment variables.
//    This is the recommended and most secure practice.
const apiKey = process.env.GEMINI_API_KEY;

if (!apiKey) {
 throw new Error("GEMINI_API_KEY environment variable not set.");
}

// 2. Instantiate the GoogleGenAI client with the API key.
const genAI = new GoogleGenAI({ apiKey });

// 3. The 'genAI' instance is now the primary interface for accessing all SDK features.
//    For example, to get a reference to a specific model:
const model = genAI.getGenerativeModel({ model: "gemini-2.5-pro" });

console.log("GoogleGenAI client initialized successfully.");

The design of the @google/genai SDK provides a significant strategic advantage by offering a unified API surface for two distinct Google Cloud backends: the developer-centric Gemini API (accessed via an API key) and the enterprise-grade Vertex AI platform. This architecture is a deliberate choice to facilitate a seamless development lifecycle. Teams can rapidly prototype applications using the often free-tiered Gemini API and, once validated, migrate the exact same codebase to a scalable, secure, and compliant Vertex AI environment with only a minor change to the client's initialization configuration.8 Instead of an
apiKey, the Vertex AI configuration requires project and location details, demonstrating the SDK's flexibility without imposing a rewrite penalty.2 This low-friction pathway from prototype to production is a powerful feature for both engineering and product strategy.


3.0 Core Architecture and Capabilities


Choosing the appropriate model is a critical architectural decision that directly impacts an application's performance, cost, and user experience. Gemini 2.5 Pro and Gemini 2.5 Flash are built on a common foundation but are optimized for different use cases. Both models are natively multimodal, capable of processing interleaved text, images, audio, and video within a single request, and both support a massive 1 million token context window.6 Their underlying architecture is a sparse Mixture-of-Experts (MoE) transformer, an advanced design that activates only a subset of the model's parameters for any given input token. This allows the models to be very large and capable while remaining computationally efficient during inference.9
The key distinction lies in their intended application and performance profiles.
Gemini 2.5 Pro is the flagship model, engineered for maximum capability and intelligence. It is the preferred choice for tasks that demand highly complex, multi-step reasoning, nuanced understanding of large and complex datasets (such as codebases or scientific documents), and advanced code generation.1 It is designed for the most demanding enterprise challenges where the accuracy and depth of the response are paramount.
Gemini 2.5 Flash, conversely, is optimized for speed, efficiency, and scale. It provides an exceptional balance of performance and cost, making it ideal for high-throughput and latency-sensitive applications. Common use cases include large-scale document summarization, responsive and interactive chatbots, real-time data extraction, and intelligent routing systems where quick, reliable responses are more critical than exhaustive reasoning.1
A significant architectural innovation shared by these models is the concept of adaptive thinking. This feature represents a shift from a monolithic inference process to a more dynamic, hybrid reasoning model. The API exposes a thinking_config parameter, allowing developers to set a thinking_budget for a given request [provided_data_models_v1]. This gives developers explicit control over the trade-off between response quality, latency, and cost. For a simple, factual query, the thinking budget can be minimized or disabled to ensure a fast, inexpensive response. For a complex analytical query, a larger budget can be allocated to enable the model to perform deeper, more extensive reasoning before generating a response.9 This level of control at the API call level is a powerful tool for building sophisticated, cost-effective, and performant AI systems.
Table 3.1: Gemini 2.5 Pro vs. Gemini 2.5 Flash Feature Comparison
Feature
	Gemini 2.5 Pro
	Gemini 2.5 Flash
	Primary Use Case
	Complex reasoning, deep analysis, advanced code generation, and demanding enterprise tasks.
	High-throughput, low-latency, and cost-sensitive applications like chatbots and large-scale summarization.
	Performance Profile
	State-of-the-art intelligence and maximum response accuracy.
	Optimized for speed, efficiency, and price-performance.
	Reasoning Capability
	The most powerful thinking model, designed for multi-step, complex problems.
	Well-rounded capabilities with support for configurable adaptive thinking via a "thinking budget".
	Cost Profile
	Premium pricing for maximum capability.
	Cost-effective, designed for high-volume tasks.
	Context Window
	1,048,576 tokens
	1,048,576 tokens
	Supported Modalities
	Audio, Images, Video, Text, PDF
	Audio, Images, Video, Text
	Key Differentiator
	Unmatched depth of reasoning and understanding for the most complex problems.
	Superior balance of speed, cost, and quality for scalable applications.
	

4.0 @google/genai SDK Reference


The @google/genai SDK is the primary interface for TypeScript developers to interact with the Gemini family of models. It is designed to be intuitive and align with modern JavaScript development patterns. The main entry point is the GoogleGenAI instance, which provides access to several specialized submodules.


4.1 The ai.models Interface


The ai.models submodule is the workhorse of the SDK, providing the core methods for sending prompts to the models.7 The primary method for standard, non-streaming requests is
generateContent. This asynchronous method takes a request object and returns a promise that resolves with the model's complete response.


TypeScript




import { GoogleGenAI, GenerateContentRequest } from "@google/genai";

const genAI = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY! });
const model = genAI.getGenerativeModel({ model: "gemini-2.5-pro" });

async function getSimpleResponse(prompt: string): Promise<string> {
 const request: GenerateContentRequest = {
   contents: [{ role: "user", parts: [{ text: prompt }] }],
 };

 const result = await model.generateContent(request);
 const response = result.response;
 
 return response.text();
}

// Example usage:
getSimpleResponse("Explain the concept of a React Hook in one paragraph.")
.then(text => console.log(text))
.catch(err => console.error(err));



4.2 Streaming & thoughtStream Implementation


For applications requiring real-time interactivity, such as chatbots or live code generation, waiting for the full response from generateContent can introduce unacceptable latency. The SDK addresses this with the generateContentStream method, which provides the technical implementation for a thoughtStream experience.2
Instead of returning a promise that resolves with the final result, generateContentStream returns an AsyncIterable stream of response chunks. This allows the application to process parts of the response as they are generated by the model, rather than waiting for the entire generation process to complete. Consuming this stream is accomplished elegantly using the for await...of loop syntax, a standard feature in modern JavaScript and TypeScript. This design choice makes the SDK feel native to the language, integrating seamlessly with the asynchronous patterns common in React development (e.g., within useEffect hooks or asynchronous event handlers) and avoiding complex callback-based logic.2 This alignment with idiomatic language features significantly lowers the learning curve and reduces the potential for bugs related to asynchronous state management.
The following snippet demonstrates the fundamental pattern for consuming a stream:


TypeScript




async function streamResponse(prompt: string) {
 const request: GenerateContentRequest = {
   contents: [{ role: "user", parts: [{ text: prompt }] }],
 };

 // This returns an AsyncIterable stream immediately.
 const streamingResult = await model.generateContentStream(request);

 // The 'for await...of' loop processes each chunk as it arrives.
 for await (const chunk of streamingResult.stream) {
   // In a real application, you would update the UI with this text chunk.
   process.stdout.write(chunk.text());
 }
 
 console.log('\nStream finished.');
}



4.3 Supporting Modules


While ai.models is the most frequently used module, the SDK provides other useful interfaces for more advanced use cases:
* ai.chats: This module simplifies the development of multi-turn conversational experiences. It provides a startChat method that returns a chat session object, which automatically manages and retains the conversation history. This abstracts away the manual process of collecting and resending the contents array with each turn of the conversation, making chatbot logic cleaner and more maintainable.10
* ai.files: For multi-modal applications, especially those that reuse large media files (images, audio, video), the ai.files module is essential. It allows developers to upload files to the Gemini API backend via the File API. The upload operation returns a file URI that can be referenced in subsequent prompts. This approach is more efficient than repeatedly sending large base64-encoded data blobs inline with each request, as it reduces bandwidth and processing overhead.2


5.0 Data Models


The Gemini API relies on a set of structured JSON objects for both requests and responses. The following tables provide a reference for the key data models.


5.1 Request Payloads


Table 5.1: GenerateContentRequest Body
Parameter
	Type
	Description
	contents
	Array
	Required. The conversational history and the latest prompt.
	tools
	Array
	Optional. A list of tools the model may use, such as function declarations or code execution.
	toolConfig
	Object
	Optional. Configuration for the tools provided in the request.
	safetySettings
	Array
	Optional. A list of settings for blocking unsafe content, overriding the default thresholds.
	generationConfig
	GenerationConfig
	Optional. Configuration options for model generation and outputs.
	systemInstruction
	Content
	Optional. High-level instructions to guide the model's behavior throughout the conversation.
	cachedContent
	string
	Optional. The name of a cached content resource to use as context.
	Table 5.2: Content Object
Parameter
	Type
	Description
	role
	string
	The role of the entity that provided this content. Must be either user or model.
	parts
	Array
	Required. An ordered list of Part objects that make up the content.
	Table 5.3: Part Object
Parameter
	Type
	Description
	text
	string
	A text string.
	inline_data
	Object
	Inline media data, containing mime_type (string) and data (base64-encoded string).
	file_data
	Object
	A reference to a file uploaded via the File API, containing mime_type (string) and file_uri (string).
	functionCall
	Object
	A function call generated by the model.
	functionResponse
	Object
	The result of a function call, provided by the client.
	executableCode
	Object
	Code generated by the model to be executed by the code execution tool.
	codeExecutionResult
	Object
	The result of executing code, provided by the code execution tool.
	Table 5.4: Tool Object
Parameter
	Type
	Description
	function_declarations
	Array
	A list of function declarations available to the model.
	code_execution
	Object
	An empty object {} to enable the code execution tool.
	Table 5.5: FunctionDeclaration Object
Parameter
	Type
	Description
	name
	string
	Required. The name of the function.
	description
	string
	Required. A description of what the function does.
	parameters
	Object
	A subset of the OpenAPI 3.0 schema describing the function's parameters.
	Table 5.6: GenerationConfig Object
Parameter
	Type
	Description
	temperature
	number
	Controls randomness. Lower values are more deterministic. Range: 0.0 to 2.0.
	topP
	number
	Nucleus sampling probability threshold.
	topK
	integer
	Top-k sampling token limit.
	candidateCount
	integer
	The number of response candidates to generate.
	maxOutputTokens
	integer
	The maximum number of tokens to generate in the response.
	stopSequences
	Array
	A list of strings that will cause the model to stop generating.
	response_mime_type
	string
	The MIME type of the output. Use application/json for structured output.
	response_schema
	Object
	The OpenAPI schema to which the response must conform when using structured output.
	thinking_config
	Object
	Configuration for the "thinking" mechanism, including thinking_budget.
	Table 5.7: SafetySetting Object
Parameter
	Type
	Description
	category
	string
	The harm category to configure (e.g., HARM_CATEGORY_HATE_SPEECH).
	threshold
	string
	The blocking threshold for this category (e.g., BLOCK_LOW_AND_ABOVE).
	

5.2 Response Objects


Table 5.8: GenerateContentResponse Body
Parameter
	Type
	Description
	candidates
	Array
	A list of response candidates generated by the model.
	promptFeedback
	Object
	Feedback on the prompt, including safety ratings.
	Table 5.9: Candidate Object
Parameter
	Type
	Description
	content
	Content
	The content of the model's response.
	finishReason
	string
	The reason the model stopped generating (e.g., STOP, MAX_TOKENS).
	safetyRatings
	Array
	A list of safety ratings for the response content.
	tokenCount
	integer
	The number of tokens in this candidate.
	

6.0 End-to-End Workflows (React/TypeScript)


This section provides complete, practical implementations of common use cases. The code is designed to be copy-paste-ready for integration into React and TypeScript projects.


6.1 Workflow 1: Implementing a Real-Time thoughtStream Component


This workflow demonstrates how to build a simple chatbot interface in React that streams the model's response in real-time, creating a "typewriter" effect that greatly enhances the user experience.
Narrative:
The component, StreamingChat, maintains the full conversation history in its state. When the user submits a message, the component appends the user's message to the history and then calls a handler function, runStream. This function initiates a streaming request to the Gemini API using generateContentStream. As each text chunk arrives from the stream, the component appends it to the last message in the history array, triggering a re-render and updating the UI incrementally. A loading state is managed to provide user feedback between the prompt submission and the arrival of the first chunk.
Complete Component Code (StreamingChat.tsx):


TypeScript




import React, { useState, useEffect } from 'react';
import { GoogleGenAI, Content } from "@google/genai";

// --- Client Initialization ---
// IMPORTANT: In a real application, this initialization should happen in a secure,
// server-side context or a dedicated service file, not directly in the component.
// The API key should never be exposed on the client side.
const apiKey = process.env.REACT_APP_GEMINI_API_KEY;
if (!apiKey) {
 throw new Error("REACT_APP_GEMINI_API_KEY is not set.");
}
const genAI = new GoogleGenAI({ apiKey });
const model = genAI.getGenerativeModel({ model: "gemini-2.5-pro" });
// --- End Client Initialization ---

interface Message {
 role: 'user' | 'model';
 text: string;
}

const StreamingChat: React.FC = () => {
 const [history, setHistory] = useState<Message>();
 const [userInput, setUserInput] = useState<string>('');
 const [isLoading, setIsLoading] = useState<boolean>(false);

 const runStream = async (prompt: string) => {
   setIsLoading(true);
   
   // Add the user's new prompt to the history
   const updatedHistory: Message = [...history, { role: 'user', text: prompt }];
   setHistory(updatedHistory);
   setUserInput('');

   // Convert message history to the format required by the API (Content)
   const apiHistory: Content = updatedHistory.map(msg => ({
     role: msg.role,
     parts: [{ text: msg.text }]
   }));
   // Remove the last user message for the API call, as it's the new prompt
   apiHistory.pop(); 

   const chat = model.startChat({
     history: apiHistory,
   });

   try {
     const result = await chat.sendMessageStream(prompt);

     let text = '';
     // Add a new, empty model message to the history to append chunks to
     setHistory(prev => [...prev, { role: 'model', text: '' }]);
     
     for await (const chunk of result.stream) {
       if (isLoading) setIsLoading(false); // Stop loading indicator on first chunk
       const chunkText = chunk.text();
       text += chunkText;
       
       // Update the last message (the model's response) in the history
       setHistory(prev => {
         const newHistory = [...prev];
         newHistory[newHistory.length - 1].text = text;
         return newHistory;
       });
     }
   } catch (error) {
     console.error("Error during streaming:", error);
     setIsLoading(false);
     // Optionally add an error message to the chat history
     setHistory(prev =>);
   }
 };

 const handleSubmit = (e: React.FormEvent) => {
   e.preventDefault();
   if (userInput.trim()) {
     runStream(userInput);
   }
 };

 return (
   <div style={{ fontFamily: 'sans-serif', maxWidth: '600px', margin: 'auto' }}>
     <h1>Real-Time Gemini Chat</h1>
     <div style={{ border: '1px solid #ccc', padding: '10px', height: '400px', overflowY: 'scroll', marginBottom: '10px' }}>
       {history.map((msg, index) => (
         <div key={index} style={{ marginBottom: '10px', textAlign: msg.role === 'user'? 'right' : 'left' }}>
           <div style={{
             display: 'inline-block',
             padding: '8px 12px',
             borderRadius: '10px',
             backgroundColor: msg.role === 'user'? '#dcf8c6' : '#f1f0f0'
           }}>
             <strong>{msg.role}:</strong> {msg.text}
           </div>
         </div>
       ))}
       {isLoading && <div>Loading...</div>}
     </div>
     <form onSubmit={handleSubmit}>
       <input
         type="text"
         value={userInput}
         onChange={(e) => setUserInput(e.target.value)}
         placeholder="Ask something..."
         style={{ width: '80%', padding: '10px' }}
         disabled={isLoading}
       />
       <button type="submit" disabled={isLoading} style={{ width: '18%', padding: '10px' }}>
         Send
       </button>
     </form>
   </div>
 );
};

export default StreamingChat;



6.2 Workflow 2: Performing Multi-modal Analysis (Image & Text)


This workflow details how to send a multi-modal prompt containing both an image and a text question to a vision-capable Gemini model. This pattern is fundamental for building applications that can "see" and reason about visual information.
Narrative:
The core of this workflow is the construction of the contents payload. The API's design for multimodality is notably elegant and simple. It does not require separate endpoints or complex request structures for different media types. Instead, it uses the parts array within a single Content object as a flexible container. Each element in the parts array is an object representing a piece of the prompt, be it text or media. This reflects the model's native ability to process these varied inputs together in a unified manner. This design significantly lowers the barrier to entry for developers, making the addition of visual understanding to a text-based application an incremental change rather than a major architectural rewrite.
The following server-side TypeScript function demonstrates this process. It reads an image file from the local filesystem, converts it to a base64 string, constructs the multi-part payload, and sends it to the model for analysis.
Complete Function Code (analyzeImage.ts):


TypeScript




import { GoogleGenAI, Part } from "@google/genai";
import * as fs from "fs";

// --- Client Initialization ---
// This should be done once in your application's setup.
const apiKey = process.env.GEMINI_API_KEY;
if (!apiKey) {
 throw new Error("GEMINI_API_KEY environment variable not set.");
}
const genAI = new GoogleGenAI({ apiKey });
// Use a model that supports vision, like gemini-2.5-pro or gemini-2.5-flash
const model = genAI.getGenerativeModel({ model: "gemini-2.5-pro" }); 
// --- End Client Initialization ---

/**
* Converts a file from a local path to a GoogleGenerativeAI.Part object.
* @param path The local path to the file.
* @param mimeType The MIME type of the file.
* @returns A Part object for the Gemini API.
*/
function fileToGenerativePart(path: string, mimeType: string): Part {
 return {
   inlineData: {
     data: Buffer.from(fs.readFileSync(path)).toString("base64"),
     mimeType,
   },
 };
}

/**
* Analyzes an image with a corresponding text prompt using a multimodal model.
* @param imagePath The local path to the image file.
* @param textPrompt The text question or instruction related to the image.
* @returns The model's text-based analysis of the image.
*/
async function analyzeImage(imagePath: string, textPrompt: string): Promise<string> {
 console.log(`Analyzing image at: ${imagePath}`);
 console.log(`With prompt: "${textPrompt}"`);

 // Define the image MIME type. Adjust if using PNG or other formats.
 const imageMimeType = "image/jpeg";

 // Construct the multimodal prompt parts
 const imagePart = fileToGenerativePart(imagePath, imageMimeType);
 const textPart = { text: textPrompt };

 const promptParts = [
   textPart,
   imagePart,
 ];

 try {
   const result = await model.generateContent({
     contents: [{ role: "user", parts: promptParts }],
   });
   
   const response = result.response;
   const analysisText = response.text();
   
   console.log("Analysis received:", analysisText);
   return analysisText;

 } catch (error) {
   console.error("Error during image analysis:", error);
   throw new Error("Failed to get analysis from the model.");
 }
}

// --- Example Usage ---
// To run this example:
// 1. Save an image as `example.jpg` in the same directory.
// 2. Run the script from your terminal (e.g., `ts-node analyzeImage.ts`).

async function main() {
 const imagePath = "example.jpg"; // Make sure this file exists
 const prompt = "Describe this image in detail. What is the main subject and what is happening?";
 
 if (!fs.existsSync(imagePath)) {
   console.error(`Error: Image file not found at ${imagePath}`);
   return;
 }

 await analyzeImage(imagePath, prompt);
}

main();



7.0 Conclusion


The Gemini 2.5 series, accessed via the modern @google/genai SDK, provides a robust and developer-friendly platform for building sophisticated AI-powered features in React and TypeScript applications. The analysis of the models and the SDK reveals several key advantages for our development teams.
First, the clear differentiation between Gemini 2.5 Pro and Gemini 2.5 Flash offers a flexible framework for architectural decisions. Teams can select Pro for applications requiring deep, complex reasoning and state-of-the-art analytical capabilities, while leveraging Flash for high-volume, latency-sensitive tasks where speed and cost-efficiency are paramount. This allows for a tailored approach, optimizing both performance and resource allocation across our product portfolio.
Second, the implementation of real-time responses through the generateContentStream method is a standout feature. The SDK's adherence to modern asynchronous JavaScript patterns, particularly the for await...of syntax, makes integrating this functionality remarkably straightforward. This enables the creation of highly engaging and responsive user interfaces, such as chatbots and live assistants, with minimal boilerplate code and a low learning curve for developers already proficient in our tech stack.
Finally, the elegant simplicity of the API's multi-modal design is a significant enabler. By treating different media types as interchangeable Part objects within a unified request structure, the SDK dramatically lowers the barrier to building rich, contextual applications that can see, hear, and reason about the world. This empowers our teams to move beyond purely text-based interactions and explore innovative use cases in visual Q&A, data analysis, and content generation.
In summary, the combination of the Gemini 2.5 models' power and the @google/genai SDK's thoughtful design equips our organization with the tools necessary to innovate and lead in the development of advanced, real-time, and multi-modal AI experiences for the web.
Works cited
1. Gemini 2.5 Updates: Flash/Pro GA, SFT, Flash-Lite on Vertex AI ..., accessed September 15, 2025, https://cloud.google.com/blog/products/ai-machine-learning/gemini-2-5-flash-lite-flash-pro-ga-vertex-ai
2. @google/genai - The GitHub pages site for the googleapis organization., accessed September 15, 2025, https://googleapis.github.io/js-genai/
3. Gemini API libraries | Google AI for Developers, accessed September 15, 2025, https://ai.google.dev/gemini-api/docs/libraries
4. google/generative-ai-docs: Documentation for Google's Gen AI site - including the Gemini API and Gemma - GitHub, accessed September 15, 2025, https://github.com/google/generative-ai-docs
5. Gemini API | Google AI for Developers, accessed September 15, 2025, https://ai.google.dev/gemini-api/docs
6. Gemini models | Gemini API | Google AI for Developers, accessed September 15, 2025, https://ai.google.dev/gemini-api/docs/models
7. @google/genai - npm, accessed September 15, 2025, https://www.npmjs.com/package/@google/genai
8. Google Gen AI SDK | Generative AI on Vertex AI - Google Cloud, accessed September 15, 2025, https://cloud.google.com/vertex-ai/generative-ai/docs/sdks/overview
9. Gemini 2.5 Flash & 2.5 Flash Image - Model Card - Googleapis.com, accessed September 15, 2025, https://storage.googleapis.com/deepmind-media/Model-Cards/Gemini-2-5-Flash-Model-Card.pdf
10. How to Use the Google Gen AI TypeScript/JavaScript SDK to Build ..., accessed September 15, 2025, https://apidog.com/blog/how-to-use-the-google-gen-ai/
11. Generate streaming text content with Generative Model | Generative AI on Vertex AI | Google Cloud, accessed September 15, 2025, https://cloud.google.com/vertex-ai/generative-ai/docs/samples/googlegenaisdk-textgen-with-txt-stream